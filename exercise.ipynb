{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4c6adc4f",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "4c6adc4f"
      },
      "source": [
        "\n",
        "\n",
        "Exercises and solutions for \n",
        "[Applied RL with RLlib](https://applied-rl-course.netlify.app/). \n",
        "The exercises found here are immediately followed by their solutions.\n",
        "\n",
        "\n",
        "**NOTE: You must run the top cells to initialize before running the exercises!**\n",
        "Keep this notebook open if you want to run multiple exercises without having to \n",
        "re-initialize. If for some reason this notebook can't request the required resources \n",
        "for Ray to run, please consider restarting the Colab Runtime and trying again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c22fea44",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "c22fea44",
        "outputId": "d64a4868-5167-4ea8-e68e-fd4b3323a237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray[rllib]==2.0.0\n",
            "  Downloading ray-2.0.0-cp38-cp38-manylinux2014_x86_64.whl (59.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Collecting grpcio<=1.43.0,>=1.28.1\n",
            "  Downloading grpcio-1.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (2.25.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (3.19.6)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (1.3.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (22.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (6.0)\n",
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.17.1-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (3.9.0)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (1.0.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (4.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (1.7.3)\n",
            "Collecting lz4\n",
            "  Downloading lz4-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (1.3.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (0.8.10)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from ray[rllib]==2.0.0) (0.18.3)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym<0.24.0,>=0.21.0\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[rllib]==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]==2.0.0) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]==2.0.0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]==2.0.0) (6.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[rllib]==2.0.0) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[rllib]==2.0.0) (5.10.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->ray[rllib]==2.0.0) (2022.7.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray[rllib]==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray[rllib]==2.0.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray[rllib]==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->ray[rllib]==2.0.0) (2.10)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]==2.0.0) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]==2.0.0) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]==2.0.0) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]==2.0.0) (2023.1.23.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]==2.0.0) (3.0)\n",
            "Requirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv->ray[rllib]==2.0.0) (2.6.2)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<0.24.0,>=0.21.0->ray[rllib]==2.0.0) (3.12.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701375 sha256=8465012d722ea3595fa171fd85b238af2e37baaf51f84f58416f602ffc2cb314\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/28/77/b0c74e80a2a4faae0161d5c53bc4f8e436e77aedc79136ee13\n",
            "Successfully built gym\n",
            "Installing collected packages: distlib, virtualenv, tensorboardX, lz4, grpcio, gym, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.51.1\n",
            "    Uninstalling grpcio-1.51.1:\n",
            "      Successfully uninstalled grpcio-1.51.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.43.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed distlib-0.3.6 grpcio-1.43.0 gym-0.23.1 lz4-4.3.2 ray-2.0.0 tensorboardX-2.5.1 virtualenv-20.17.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.23.1 in /usr/local/lib/python3.8/dist-packages (0.23.1)\n",
            "Collecting gym-toytext==0.25.0\n",
            "  Downloading gym_toytext-0.25.0-py3-none-any.whl (10 kB)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym==0.23.1) (3.12.0)\n",
            "Installing collected packages: pygame, gym-toytext\n",
            "Successfully installed gym-toytext-0.25.0 pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "# RUN THESE THREE CELLS FIRST\n",
        "! pip install ray[rllib]==2.0.0 torch matplotlib \n",
        "! pip install gym==0.23.1 gym-toytext==0.25.0 pygame==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "179cd3bb",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "179cd3bb",
        "outputId": "2315ecac-99c0-4892-8dc9-941c547a8eac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rl-course-exercises'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 160 (delta 63), reused 128 (delta 34), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (160/160), 10.43 MiB | 18.11 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ],
      "source": [
        "# RUN THESE THREE CELLS FIRST\n",
        "!git clone https://github.com/maxpumperla/rl-course-exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ae121a53",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "ae121a53"
      },
      "outputs": [],
      "source": [
        "# RUN THESE THREE CELLS FIRST\n",
        "%cp -r rl-course-exercises/utils.py .\n",
        "%cp -r rl-course-exercises/envs.py .\n",
        "\n",
        "%cp -r rl-course-exercises/data .\n",
        "%cp -r rl-course-exercises/models ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ac20eee",
      "metadata": {
        "id": "3ac20eee"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 3 from module 1\n",
        "from ray.rllib.algorithms.ppo import PPO\n",
        "from utils import slippery_algo_config\n",
        "import gym\n",
        "from IPython import display\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
        "ppo.restore(\"models/FrozenLakeSlippery50/checkpoint_000050/\")\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "from utils import fix_frozen_lake_render\n",
        "fix_frozen_lake_render(env)\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "# The player behaves randomly by default.\n",
        "# To use a player that learned how to play using RL, change this to True!\n",
        "USE_RL = False\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    if USE_RL:\n",
        "        action = ppo.compute_single_action(obs, explore=False)\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    env.render()\n",
        "    time.sleep(0.25)\n",
        "    \n",
        "if reward == 1:\n",
        "    print(\"Success!\")\n",
        "else:\n",
        "    print(\"Agent failed to reach the goal :(\")\n",
        "    \n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e134fd",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "83e134fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 7 from module 1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "env = gym.make(\"FrozenLake-v1\", \n",
        "               desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), \n",
        "               is_slippery=False)\n",
        "env.render = None\n",
        "\n",
        "obs = env.reset()\n",
        "actions = [____]\n",
        "for action in actions:\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55387e4f",
      "metadata": {
        "id": "55387e4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 7 from module 1\n",
        "\n",
        "import gym.envs.toy_text\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
        "env.render = None\n",
        "\n",
        "obs = env.reset()\n",
        "actions = [1,2,1,2]\n",
        "for action in actions:\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30c7fa6",
      "metadata": {
        "id": "b30c7fa6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 10 from module 1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "rewards = []\n",
        "N = 1000\n",
        "\n",
        "for ____:\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while ____:\n",
        "        action = np.random.randint(low=0, high=4)\n",
        "        obs, reward, done, _ = env.step(____)\n",
        "    \n",
        "    rewards.append(reward)\n",
        "    \n",
        "print(\"Average reward:\", sum(rewards)/N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab85f88",
      "metadata": {
        "id": "fab85f88"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 10 from module 1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "rewards = []\n",
        "N = 1000\n",
        "\n",
        "for i in range(N): # loop over N episodes\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.random.randint(low=0, high=4)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "    \n",
        "    rewards.append(reward)\n",
        "    \n",
        "print(\"Average reward:\", sum(rewards)/N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f13db28",
      "metadata": {
        "id": "9f13db28"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 11 from module 1\n",
        "\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "# Your task: modify this dictionary\n",
        "policy = {\n",
        "    0 : 0,\n",
        "    1 : 0,\n",
        "    2 : 0,\n",
        "    3 : 0,\n",
        "    4 : 0,\n",
        "    5 : 0,\n",
        "    6 : 0,\n",
        "    7 : 0,\n",
        "    8 : 0,\n",
        "    9 : 0,\n",
        "    10: 0,\n",
        "    11: 0,\n",
        "    12: 0,\n",
        "    13: 0,\n",
        "    14: 0,\n",
        "    15: 0\n",
        "}\n",
        "\n",
        "rewards = []\n",
        "N = 1000\n",
        "for i in range(N): # loop over N episodes\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = policy[obs]\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "    \n",
        "    rewards.append(reward)\n",
        "    \n",
        "print(\"Average reward:\", sum(rewards)/N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab3872f",
      "metadata": {
        "id": "1ab3872f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 11 from module 1\n",
        "\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "\n",
        "policy = {\n",
        "    0 : 2,\n",
        "    1 : 2,\n",
        "    2 : 2,\n",
        "    3 : 2,\n",
        "    4 : 1,\n",
        "    5 : 1,\n",
        "    6 : 1,\n",
        "    7 : 1,\n",
        "    8 : 2,\n",
        "    9 : 2,\n",
        "    10: 2,\n",
        "    11: 0,\n",
        "    12: 2,\n",
        "    13: 2,\n",
        "    14: 2,\n",
        "    15: 2\n",
        "}\n",
        "\n",
        "rewards = []\n",
        "N = 1000\n",
        "for i in range(N): # loop over N episodes\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = policy[obs]\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "    \n",
        "    rewards.append(reward)\n",
        "    \n",
        "print(\"Average reward:\", sum(rewards)/N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0f2b0cc5",
      "metadata": {
        "id": "0f2b0cc5",
        "outputId": "91af89dc-4745-4222-c29b-e55dabb13efb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1843)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=1843)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=1844)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=1844)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1844)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1844)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1844)\u001b[0m /usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1844)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n",
            "2023-02-05 21:30:27,931\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1843)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1843)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1843)\u001b[0m /usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1843)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8076923076923077\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# exercise number 3 from module 2\n",
        "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
        "from utils import slippery_algo_config\n",
        "\n",
        "ppo = PPO(env='FrozenLake-v1', config=slippery_algo_config)\n",
        "\n",
        "for i in range(50):\n",
        "    train_info = ppo.train()\n",
        "    \n",
        "eval_results = ppo.evaluate()\n",
        "\n",
        "print(eval_results[\"evaluation\"][\"episode_reward_mean\"])\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7bc5ffec",
      "metadata": {
        "id": "7bc5ffec",
        "outputId": "e2582097-ffda-4af7-e39f-3ed666ce9612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=4014)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=4014)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=4013)\u001b[0m /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=4013)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4014)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4014)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4014)\u001b[0m /usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4014)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4013)\u001b[0m /usr/local/lib/python3.8/dist-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4013)\u001b[0m   deprecation(\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4013)\u001b[0m /usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=4013)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n",
            "2023-02-05 21:38:51,462\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of reaching goal: 75.5%\n",
            "Action performed from top-right: 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# solution for exercise number 3 from module 2\n",
        "from ray.rllib.algorithms.ppo import PPO\n",
        "from utils import slippery_algo_config\n",
        "\n",
        "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
        "\n",
        "for i in range(50): # There is randomness here, but 20+ should be enough most of the time\n",
        "    train_info = ppo.train()\n",
        "    \n",
        "eval_results = ppo.evaluate()\n",
        "\n",
        "print(\"Frequency of reaching goal: %.1f%%\" % (eval_results[\"evaluation\"][\"episode_reward_mean\"]*100))\n",
        "\n",
        "print(\"Action performed from top-right:\", ppo.compute_single_action(3, explore=False))\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5d04609e",
      "metadata": {
        "id": "5d04609e",
        "outputId": "d0c05c57-b282-4c75-9f77-3d3d24e2e97e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "....\n",
            ".O.O\n",
            "...O\n",
            "O..P\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# exercise number 4 from module 2\n",
        "from ray.rllib.algorithms.ppo import PPO\n",
        "from utils import slippery_algo_config\n",
        "import gym\n",
        "from IPython import display\n",
        "import time\n",
        "\n",
        "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
        "ppo.restore(\"models/FrozenLakeSlippery50/checkpoint_000050/\")\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "from utils import fix_frozen_lake_render\n",
        "fix_frozen_lake_render(env)\n",
        "\n",
        "obs = env.reset()\n",
        "env.seed(12)\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = ppo.compute_single_action(obs, explore=False)\n",
        "    obs, rewards, done, _ = env.step(action)\n",
        "\n",
        "    display.clear_output(wait=True);\n",
        "    env.render()\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e783d8f6",
      "metadata": {
        "id": "e783d8f6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 8 from module 2\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display\n",
        "\n",
        "from envs import MyCartPole\n",
        "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
        "\n",
        "config = (\n",
        "    ____()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
        "    .environment(env=MyCartPole)\n",
        ")\n",
        "\n",
        "ppo = ____.____()\n",
        "\n",
        "____.restore(\"models/CartPole-Ray2/checkpoint_000050\")\n",
        "\n",
        "env = MyCartPole()\n",
        "obs = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = ppo.compute_single_action(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    env.render()\n",
        "    time.sleep(0.01)\n",
        "\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15070c1",
      "metadata": {
        "id": "b15070c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 8 from module 2\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display\n",
        "\n",
        "from envs import MyCartPole\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
        "    .environment(env=MyCartPole)\n",
        ")\n",
        "\n",
        "ppo = config.build()\n",
        "\n",
        "ppo.restore(\"models/CartPole-Ray2/checkpoint_000050\")\n",
        "\n",
        "env = MyCartPole()\n",
        "obs = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = ppo.compute_single_action(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    env.render()\n",
        "    time.sleep(0.01)\n",
        "\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316f228a",
      "metadata": {
        "id": "316f228a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 9 from module 2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import utils\n",
        "from envs import MyCartPole\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "cartpole_config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
        "    .environment(env=MyCartPole)\n",
        ")\n",
        "\n",
        "ppo = cartpole_config.build()\n",
        "\n",
        "ppo.restore(\"models/CartPole-Ray2/checkpoint_000050\")\n",
        "\n",
        "angle_range_deg = np.arange(-15,15,0.1)\n",
        "push_left_probs = 0*angle_range_deg\n",
        "\n",
        "env = MyCartPole()\n",
        "obs = env.reset()\n",
        "for i, angle_deg in enumerate(angle_range_deg):\n",
        "    angle_rad = angle_deg/180*np.pi\n",
        "    \n",
        "    obs = np.zeros(4)\n",
        "    obs[2] = angle_rad\n",
        "\n",
        "    push_left_probs[i] = utils.query_policy(ppo, env, obs, actions=[0,1])[0]\n",
        "\n",
        "plt.plot(angle_range_deg, push_left_probs)\n",
        "plt.xlabel(\"pole angle (degrees)\")\n",
        "plt.ylabel(\"probability of pushing left\")\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73655c0",
      "metadata": {
        "id": "e73655c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 13 from module 2\n",
        "\n",
        "from envs import MultiAgentArena\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "import time\n",
        "\n",
        "ppo_config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
        "    .environment(env=MultiAgentArena)\n",
        "    .multi_agent(\n",
        "        ____=[\"policy1\", \"policy2\"],\n",
        "        ____=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ppo_arena = ppo_config.build()\n",
        "\n",
        "try: # Restore depending on Python version\n",
        "    ppo_arena.restore(\"models/MultiAgent20/checkpoint_000020\")\n",
        "except TypeError:\n",
        "    ppo_arena.restore(\"models/MultiAgent20py37/checkpoint_000020\")\n",
        "\n",
        "env = MultiAgentArena(config={\"render\": True})\n",
        "obs = env.reset()\n",
        "dones = {\"__all__\" : False}\n",
        "    \n",
        "while not dones[\"__all__\"]:\n",
        "\n",
        "    action1 = ppo_arena.compute_single_action(____, policy_id=\"policy1\")\n",
        "    action2 = ppo_arena.compute_single_action(____, policy_id=\"policy2\")\n",
        "\n",
        "    obs, rewards, dones, infos = env.step({\"agent1\": ____, \"agent2\": ____})\n",
        "\n",
        "    env.render()\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "ppo_arena.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6b7165",
      "metadata": {
        "id": "dd6b7165"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 13 from module 2\n",
        "from envs import MultiAgentArena\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "import time\n",
        "\n",
        "ppo_config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [64, 64]})\n",
        "    .environment(env=MultiAgentArena)\n",
        "    .multi_agent(\n",
        "        policies=[\"policy1\", \"policy2\"],\n",
        "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ppo_arena = ppo_config.build()\n",
        "\n",
        "try: # Restore depending on Python version\n",
        "    ppo_arena.restore(\"models/MultiAgent20/checkpoint_000020\")\n",
        "except TypeError:\n",
        "    ppo_arena.restore(\"models/MultiAgent20py37/checkpoint_000020\")\n",
        "\n",
        "env = MultiAgentArena(config={\"render\": True})\n",
        "obs = env.reset()\n",
        "dones = {\"__all__\" : False}\n",
        "    \n",
        "while not dones[\"__all__\"]:\n",
        "\n",
        "    action1 = ppo_arena.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
        "    action2 = ppo_arena.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
        "\n",
        "    obs, rewards, dones, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
        "\n",
        "    env.render()\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "ppo_arena.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbebd3e",
      "metadata": {
        "id": "ddbebd3e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 3 from module 3\n",
        "from envs import FrozenPond\n",
        "\n",
        "\n",
        "class Maze(FrozenPond):\n",
        "    def done(self):\n",
        "        return self.player == self.goal or self.holes[self.player] == 1\n",
        "    def is_valid_loc(self, location):\n",
        "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "pond = FrozenPond()\n",
        "pond.reset()\n",
        "pond.step(1)\n",
        "print(pond.step(2))\n",
        "\n",
        "maze = Maze()\n",
        "maze.reset()\n",
        "maze.step(1)\n",
        "print(maze.step(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ee9049",
      "metadata": {
        "id": "b3ee9049"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 3 from module 3\n",
        "from envs import FrozenPond\n",
        "\n",
        "\n",
        "class Maze(FrozenPond):   \n",
        "    def done(self):\n",
        "        return self.player == self.goal\n",
        "    def is_valid_loc(self, location):\n",
        "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "pond = FrozenPond()\n",
        "pond.reset()\n",
        "pond.step(1)\n",
        "print(pond.step(2))\n",
        "\n",
        "maze = Maze()\n",
        "maze.reset()\n",
        "maze.step(1)\n",
        "print(maze.step(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e81606",
      "metadata": {
        "id": "03e81606"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 8 from module 3\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "class RandomLake(gym.Env):\n",
        "    def __init__(self, env_config=None):\n",
        "        self.observation_space = gym.spaces.Discrete(16)\n",
        "        self.action_space = gym.spaces.Discrete(4)      \n",
        "        \n",
        "    def reset(self):\n",
        "        self.player = (0, 0) # the player starts at the top-left\n",
        "        self.goal = (3, 3)   # goal is at the bottom-right\n",
        "        \n",
        "        self.holes = np.random.rand(4, 4) < 0.2\n",
        "        self.holes[self.player] = 0  # no hole at start location\n",
        "        self.holes[self.goal] = 0    # no hole at goal location\n",
        "                \n",
        "        return self.observation()\n",
        "    \n",
        "    def observation(self):\n",
        "        return 4*self.player[0] + self.player[1]\n",
        "    \n",
        "    def reward(self):\n",
        "        return int(self.player == self.goal)\n",
        "    \n",
        "    def done(self):\n",
        "        is_done = self.player == self.goal or self.holes[self.player] == 1 \n",
        "        return is_done\n",
        "    \n",
        "    def is_valid_loc(self, location):\n",
        "        return 0 <= location[0] <= 3 and 0 <= location[1] <= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        # Compute the new player location\n",
        "        if action == 0:   # left\n",
        "            new_loc = (self.player[0], self.player[1]-1)\n",
        "        elif action == 1: # down\n",
        "            new_loc = (self.player[0]+1, self.player[1])\n",
        "        elif action == 2: # right\n",
        "            new_loc = (self.player[0], self.player[1]+1)\n",
        "        elif action == 3: # up\n",
        "            new_loc = (self.player[0]-1, self.player[1])\n",
        "        else:\n",
        "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
        "        \n",
        "        # Update the player location only if you stayed in bounds\n",
        "        if self.is_valid_loc(new_loc):\n",
        "            self.player = new_loc\n",
        "                    \n",
        "        return self.observation(), self.reward(), self.done(), {}\n",
        "    \n",
        "lake = RandomLake()\n",
        "obs = lake.reset()\n",
        "\n",
        "done = False\n",
        "for i in range(55):\n",
        "    obs, rewards, done, _ = lake.step(0)\n",
        "    print(i+1, done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b47d3e",
      "metadata": {
        "id": "83b47d3e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 8 from module 3\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "class RandomLake(gym.Env):\n",
        "    def __init__(self, env_config=None):\n",
        "        self.observation_space = gym.spaces.Discrete(16)\n",
        "        self.action_space = gym.spaces.Discrete(4)      \n",
        "        \n",
        "    def reset(self):\n",
        "        self.player = (0, 0) # the player starts at the top-left\n",
        "        self.goal = (3, 3)   # goal is at the bottom-right\n",
        "        \n",
        "        self.holes = np.random.rand(4, 4) < 0.2\n",
        "        self.holes[self.player] = 0  # no hole at start location\n",
        "        self.holes[self.goal] = 0    # no hole at goal location\n",
        "        \n",
        "        self.stepcount = 0\n",
        "        \n",
        "        return self.observation()\n",
        "    \n",
        "    def observation(self):\n",
        "        return 4*self.player[0] + self.player[1]\n",
        "    \n",
        "    def reward(self):\n",
        "        return int(self.player == self.goal)\n",
        "    \n",
        "    def done(self):\n",
        "        is_done = self.player == self.goal or self.holes[self.player] == 1 or self.stepcount >= 50\n",
        "        return is_done\n",
        "    \n",
        "    def is_valid_loc(self, location):\n",
        "        return 0 <= location[0] <= 3 and 0 <= location[1] <= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        # Compute the new player location\n",
        "        if action == 0:   # left\n",
        "            new_loc = (self.player[0], self.player[1]-1)\n",
        "        elif action == 1: # down\n",
        "            new_loc = (self.player[0]+1, self.player[1])\n",
        "        elif action == 2: # right\n",
        "            new_loc = (self.player[0], self.player[1]+1)\n",
        "        elif action == 3: # up\n",
        "            new_loc = (self.player[0]-1, self.player[1])\n",
        "        else:\n",
        "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
        "        \n",
        "        # Update the player location only if you stayed in bounds\n",
        "        if self.is_valid_loc(new_loc):\n",
        "            self.player = new_loc\n",
        "            \n",
        "        self.stepcount += 1\n",
        "        \n",
        "        return self.observation(), self.reward(), self.done(), {}\n",
        "    \n",
        "lake = RandomLake()\n",
        "obs = lake.reset()\n",
        "\n",
        "done = False\n",
        "for i in range(55):\n",
        "    obs, rewards, done, _ = lake.step(0)\n",
        "    print(i+1, done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28be7c6",
      "metadata": {
        "id": "b28be7c6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 13 from module 3\n",
        "import numpy as np\n",
        "from envs import RandomLakeObs\n",
        "\n",
        "class RandomLakeObs2(RandomLakeObs):\n",
        "    def observation(self):\n",
        "        i, j = self.player\n",
        "\n",
        "        obs = [1 if j == 0 else self.holes[i, j - 1],\n",
        "               1 if i == 3 else self.holes[i + 1, j],\n",
        "               1 if j == 3 else self.holes[i, j + 1],\n",
        "               1 if i == 0 else self.holes[i - 1, j]]\n",
        "\n",
        "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
        "        return obs\n",
        "\n",
        "np.random.seed(42)\n",
        "env = RandomLakeObs2()\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "print(obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf9664dc",
      "metadata": {
        "id": "cf9664dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 13 from module 3\n",
        "import numpy as np\n",
        "from envs import RandomLakeObs\n",
        "\n",
        "class RandomLakeObs2(RandomLakeObs):\n",
        "    def observation(self):\n",
        "        i, j = self.player\n",
        "\n",
        "        obs = []\n",
        "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
        "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
        "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
        "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
        "        \n",
        "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
        "        return obs\n",
        "\n",
        "np.random.seed(42)\n",
        "env = RandomLakeObs2()\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "print(obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a290e774",
      "metadata": {
        "id": "a290e774"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 14 from module 3\n",
        "\n",
        "import numpy as np\n",
        "from envs import RandomLakeObs\n",
        "\n",
        "actions = {\"left\": 0, \"down\": 1, \"right\": 2, \"up\": 3,\n",
        "           \"l\": 0, \"d\": 1, \"r\": 2, \"u\": 3}\n",
        "\n",
        "np.random.seed(45)\n",
        "env = RandomLakeObs()\n",
        "obs = env.reset()\n",
        "\n",
        "act = \"start\"\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "   \n",
        "    obs_print = [['.']*3 for i in range(3)]\n",
        "    obs_print[1][1] = \"P\"\n",
        "    if obs[0]:\n",
        "        obs_print[1][0] = \"O\"\n",
        "    if obs[1]:\n",
        "        obs_print[2][1] = \"O\"\n",
        "    if obs[2]:\n",
        "        obs_print[1][2] = \"O\"\n",
        "    if obs[3]:\n",
        "        obs_print[0][1] = \"O\"\n",
        "    print(\"Observation (what the agent sees):\")\n",
        "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
        "    print()\n",
        "    \n",
        "    while act != \"quit\" and act not in actions: \n",
        "        act = input() # gather keyboard input \n",
        "    \n",
        "    if act == \"quit\":\n",
        "        break\n",
        "        \n",
        "    obs, rew, done, _ = env.step(actions[act])\n",
        "    act = None\n",
        "    \n",
        "if done:\n",
        "    if rew > 0:\n",
        "        print(\"You win! +1 reward 🎉\")\n",
        "    else:\n",
        "        print(\"You fell into the lake 😢\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8b5e7b",
      "metadata": {
        "id": "bf8b5e7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 19 from module 3\n",
        "from utils import lake_default_config\n",
        "from envs import RandomLakeObs\n",
        "\n",
        "class RandomLakeBadIdea(RandomLakeObs):\n",
        "    def reward(self):\n",
        "        old_reward = int(self.player == self.goal) \n",
        "        return ____\n",
        "    \n",
        "ppo = lake_default_config.build(env=____)\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    ppo.train()\n",
        "    \n",
        "print(\"Average episode length for trained agent: %.1f\" % \n",
        "      ppo.evaluate()[\"evaluation\"][____])\n",
        "\n",
        "random_agent_config = (\n",
        "    lake_default_config\n",
        "    .exploration(exploration_config={\"type\": \"Random\"})\n",
        "    .evaluation(evaluation_config={\"explore\" : True})\n",
        ")\n",
        "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
        "\n",
        "print(\"Average episode length for random agent: %.1f\" % \n",
        "      random_agent.evaluate()[\"evaluation\"][____])\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193a31ed",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "193a31ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 19 from module 3\n",
        "from utils import lake_default_config\n",
        "from envs import RandomLakeObs\n",
        "\n",
        "class RandomLakeBadIdea(RandomLakeObs):\n",
        "    def reward(self):\n",
        "        old_reward = int(self.player == self.goal) \n",
        "        return old_reward - 1\n",
        "\n",
        "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    ppo.train()\n",
        "    \n",
        "print(\"Average episode length for trained agent: %.1f\" % \n",
        "      ppo.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
        "\n",
        "random_agent_config = (\n",
        "    lake_default_config\n",
        "    .exploration(exploration_config={\"type\": \"Random\"})\n",
        "    .evaluation(evaluation_config={\"explore\" : True})\n",
        ")\n",
        "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
        "\n",
        "print(\"Average episode length for random agent: %.1f\" % \n",
        "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5061e6d8",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5061e6d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 8 from module 4\n",
        "\n",
        "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
        "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
        "\n",
        "def reward(sugar_level, item_sweetness):\n",
        "    return item_sweetness * (1 - sugar_level)\n",
        "\n",
        "# MODIFY THIS LIST\n",
        "# But make sure it always contains 3 items, each 0 or 1\n",
        "recommendations = [0, 0, 0]\n",
        "\n",
        "# starting sugar level\n",
        "sugar_level = 0.5\n",
        "\n",
        "total_reward = 0\n",
        "\n",
        "for item_sweetness in recommendations:\n",
        "    \n",
        "    # add reward\n",
        "    immediate_reward = reward(sugar_level, item_sweetness)\n",
        "    total_reward += immediate_reward\n",
        "    \n",
        "    # update sugar level\n",
        "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
        "    \n",
        "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
        "    \n",
        "print(\"Total reward after 5 recommendations:\", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d66b9fc2",
      "metadata": {
        "id": "d66b9fc2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 8 from module 4\n",
        "\n",
        "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
        "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
        "\n",
        "def reward(sugar_level, item_sweetness):\n",
        "    return item_sweetness * (1 - sugar_level)\n",
        "\n",
        "# MODIFY THIS LIST\n",
        "# But make sure it always contains 3 items, each 0 or 1\n",
        "recommendations = [0,1,1]\n",
        "\n",
        "# starting sugar level\n",
        "sugar_level = 0.5\n",
        "\n",
        "total_reward = 0\n",
        "\n",
        "for item_sweetness in recommendations:\n",
        "    \n",
        "    # add reward\n",
        "    immediate_reward = reward(sugar_level, item_sweetness)\n",
        "    total_reward += immediate_reward\n",
        "    \n",
        "    # update sugar level\n",
        "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
        "    \n",
        "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
        "    \n",
        "print(\"Total reward after 5 recommendations\", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e48d171f",
      "metadata": {
        "id": "e48d171f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 12 from module 4\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from envs import BasicRecommender\n",
        "\n",
        "def baseline_episode(env, method=\"greedy\"):\n",
        "    \"\"\"\n",
        "    Compute the episode reward for a BasicRecommender env by either\n",
        "    acting greedy (max observation) or acting randomly.\n",
        "    Return total reward.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        if method == \"greedy\":\n",
        "            chosen_item = np.argmax(____)\n",
        "        elif method == \"random\":\n",
        "            chosen_item = np.random.randint(____)\n",
        "        else:\n",
        "            raise Exception(\"Unknown method.\")\n",
        "        obs, reward, done, info = env.____(chosen_item)\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "def baseline_multiple_episodes(env, method, n_ep=100):\n",
        "    \"\"\" Compute baseline reward averaged over multiple episodes \"\"\"\n",
        "    return np.mean([baseline_episode(env, method) for ep in range(n_ep)])\n",
        "\n",
        "max_steps = [1,3,10,30,100] # The different horizon lengths to test out\n",
        "greedy_results = []\n",
        "random_results = []\n",
        "\n",
        "# Run the simulations\n",
        "for ms in ____:\n",
        "    env = BasicRecommender({\"max_steps\" : ____})\n",
        "    greedy_results.append(baseline_multiple_episodes(env, \"greedy\"))\n",
        "    random_results.append(baseline_multiple_episodes(env, \"random\"))\n",
        "    \n",
        "# Plotting code (you can ignore)\n",
        "plt.plot(max_steps, greedy_results, label=\"greedy\")\n",
        "plt.plot(max_steps, random_results, label=\"random\")\n",
        "plt.legend();\n",
        "plt.xlabel(\"horizon of the env\");\n",
        "plt.ylabel(\"episode reward\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee58fab1",
      "metadata": {
        "id": "ee58fab1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 12 from module 4\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from envs import BasicRecommender\n",
        "\n",
        "def baseline_episode(env, method=\"greedy\"):\n",
        "    \"\"\"\n",
        "    Compute the episode reward for a BasicRecommender env by either\n",
        "    acting greedy (max observation) or acting randomly.\n",
        "    Return total reward.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        if method == \"greedy\":\n",
        "            chosen_item = np.argmax(obs)\n",
        "        elif method == \"random\":\n",
        "            chosen_item = np.random.randint(len(obs))\n",
        "        else:\n",
        "            raise Exception(\"Unknown method.\")\n",
        "        obs, reward, done, info = env.step(chosen_item)\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "def baseline_multiple_episodes(env, method, n_ep=100):\n",
        "    \"\"\" Compute baseline reward averaged over multiple episodes \"\"\"\n",
        "    return np.mean([baseline_episode(env, method) for ep in range(n_ep)])\n",
        "\n",
        "max_steps = [1,3,10,30,100] # The different horizon lengths to test out\n",
        "greedy_results = []\n",
        "random_results = []\n",
        "\n",
        "# Run the simulations\n",
        "for ms in max_steps:\n",
        "    env = BasicRecommender({\"max_steps\" : ms})\n",
        "    greedy_results.append(baseline_multiple_episodes(env, \"greedy\"))\n",
        "    random_results.append(baseline_multiple_episodes(env, \"random\"))\n",
        "    \n",
        "# Plotting code (you can ignore)\n",
        "plt.plot(max_steps, greedy_results, label=\"greedy\")\n",
        "plt.plot(max_steps, random_results, label=\"random\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"horizon of the env\")\n",
        "plt.ylabel(\"episode reward\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f1b75e3",
      "metadata": {
        "id": "9f1b75e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 15 from module 4\n",
        "import numpy as np\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from envs import BasicRecommender\n",
        "from utils import query_policy\n",
        "\n",
        "env_config = {\n",
        "    \"num_candidates\" : 2,\n",
        "    \"alpha\"          : 0.5,\n",
        "    \"seed\"           : 42\n",
        "}\n",
        "\n",
        "ppo_config = ( # change \"gamma\" in here\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [64, 64]}, lr=0.001, gamma=0.99)\n",
        "    .environment(env=BasicRecommender, env_config=env_config)\n",
        ")\n",
        "\n",
        "ppo = ppo_config.build()\n",
        "\n",
        "rewards = []\n",
        "for i in range(5):\n",
        "    result = ppo.train()\n",
        "    rewards.append(result[\"episode_reward_mean\"])\n",
        "\n",
        "env = BasicRecommender(env_config)\n",
        "env.reset()\n",
        "print(query_policy(ppo, env, np.array([0,1]), actions=[0,1]))\n",
        "\n",
        "ppo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1a0457",
      "metadata": {
        "id": "ed1a0457"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 4 from module 5\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ____ import ____\n",
        "\n",
        "# suppress warnings\n",
        "import ray\n",
        "import logging\n",
        "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "ppo_config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
        "    .environment(env=\"Taxi-v3\")\n",
        ")\n",
        "\n",
        "ppo = ppo_config.build()\n",
        "\n",
        "ppo_rewards = []\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    out = ppo.train()\n",
        "    ppo_rewards.append(out[\"episode_reward_mean\"])\n",
        "    \n",
        "\n",
        "dqn_config = (\n",
        "    ____()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
        "    .environment(env=\"Taxi-v3\")\n",
        ")\n",
        "\n",
        "dqn = dqn_config.build()\n",
        "\n",
        "dqn_rewards = []\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    out = ____.train()\n",
        "    dqn_rewards.append(out[\"episode_reward_mean\"])\n",
        "    \n",
        "plt.plot(ppo_rewards, label=\"PPO\")\n",
        "plt.plot(dqn_rewards, label=\"DQN\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"training iterations\")\n",
        "plt.ylabel(\"reward\")\n",
        "\n",
        "ppo.stop()\n",
        "dqn.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead944f8",
      "metadata": {
        "id": "ead944f8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 4 from module 5\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "\n",
        "# suppress warnings\n",
        "import ray\n",
        "import logging\n",
        "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "ppo_config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
        "    .environment(env=\"Taxi-v3\")\n",
        ")\n",
        "\n",
        "ppo = ppo_config.build()\n",
        "\n",
        "ppo_rewards = []\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    out = ppo.train()\n",
        "    ppo_rewards.append(out[\"episode_reward_mean\"])\n",
        "    \n",
        "\n",
        "dqn_config = (\n",
        "    DQNConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
        "    .environment(env=\"Taxi-v3\")\n",
        ")\n",
        "\n",
        "dqn = dqn_config.build()\n",
        "\n",
        "dqn_rewards = []\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    out = dqn.train()\n",
        "    dqn_rewards.append(out[\"episode_reward_mean\"])\n",
        "    \n",
        "plt.plot(ppo_rewards, label=\"PPO\")\n",
        "plt.plot(dqn_rewards, label=\"DQN\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"training iterations\")\n",
        "plt.ylabel(\"reward\")\n",
        "\n",
        "ppo.stop()\n",
        "dqn.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "239bce91",
      "metadata": {
        "id": "239bce91"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 9 from module 5\n",
        "\n",
        "from envs import BasicRecommenderWithHistory\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray import tune\n",
        "\n",
        "env_config = {\n",
        "    \"num_candidates\" : 2,\n",
        "    \"alpha\"          : 0.5,\n",
        "    \"seed\"           : 42\n",
        "}\n",
        "\n",
        "ppo_config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True, num_rollout_workers=0)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
        "              lr=____)\n",
        "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
        ")\n",
        "\n",
        "analysis = tune.____(\n",
        "    \"PPO\",\n",
        "    config            = ppo_config.to_dict(),\n",
        "    ____              = {\"training_iteration\" : 10},\n",
        "    checkpoint_freq   = 1,\n",
        "    verbose           = 0,\n",
        "    metric            = \"episode_reward_mean\",\n",
        "    mode              = \"max\"\n",
        ")\n",
        "\n",
        "____.results_df[[\"lr\", \"episode_reward_mean\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570e414a",
      "metadata": {
        "id": "570e414a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 9 from module 5\n",
        "\n",
        "from envs import BasicRecommenderWithHistory\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray import tune\n",
        "\n",
        "env_config = {\n",
        "    \"num_candidates\" : 2,\n",
        "    \"alpha\"          : 0.5,\n",
        "    \"seed\"           : 42\n",
        "}\n",
        "\n",
        "ppo_config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(create_env_on_local_worker=True, num_rollout_workers=0)\n",
        "    .debugging(seed=0, log_level=\"ERROR\")\n",
        "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
        "              lr=tune.grid_search([1e-2, 1e-3, 1e-4]))\n",
        "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
        ")\n",
        "\n",
        "analysis = tune.run(\n",
        "    \"PPO\",\n",
        "    config            = ppo_config.to_dict(),\n",
        "    stop              = {\"training_iteration\" : 10},\n",
        "    checkpoint_freq   = 1,\n",
        "    verbose           = 0,\n",
        "    metric            = \"episode_reward_mean\",\n",
        "    mode              = \"max\"\n",
        ")\n",
        "\n",
        "analysis.results_df[[\"config/lr\", \"episode_reward_mean\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d5454f",
      "metadata": {
        "id": "41d5454f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# exercise number 13 from module 5\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "ppo_config_many = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .____(____)\n",
        "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
        "    .environment(env=\"FrozenLake-v1\")\n",
        ")\n",
        "\n",
        "ppo_config_single = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .____(num_rollout_workers=1, num_envs_per_worker=1)\n",
        "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
        "    .environment(env=\"FrozenLake-v1\")\n",
        ")\n",
        "\n",
        "ppo_many = ppo_config_many.build()\n",
        "t = time.time()\n",
        "for i in range(5):\n",
        "    ppo_many.train()\n",
        "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
        "ppo_many.stop()\n",
        "\n",
        "ppo_single = ppo_config_single.build()\n",
        "t = time.time()\n",
        "for i in range(5):\n",
        "    ppo_single.train()\n",
        "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
        "ppo_single.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a7d1a4",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "77a7d1a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# solution for exercise number 13 from module 5\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "ppo_config_many = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(num_rollout_workers=2, num_envs_per_worker=2)\n",
        "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
        "    .environment(env=\"FrozenLake-v1\")\n",
        ")\n",
        "\n",
        "ppo_config_single = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\n",
        "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
        "    .environment(env=\"FrozenLake-v1\")\n",
        ")\n",
        "\n",
        "ppo_many = ppo_config_many.build()\n",
        "t = time.time()\n",
        "for i in range(5):\n",
        "    ppo_many.train()\n",
        "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
        "ppo_many.stop()\n",
        "\n",
        "ppo_single = ppo_config_single.build()\n",
        "t = time.time()\n",
        "for i in range(5):\n",
        "    ppo_single.train()\n",
        "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
        "ppo_single.stop()"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}